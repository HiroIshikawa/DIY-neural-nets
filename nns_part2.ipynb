{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the numpy library.\n",
    "\n",
    "To get familar with what is numpy and how to use it, I recommned to go over this tutorial:\n",
    "https://docs.scipy.org/doc/numpy-dev/user/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute sigmoid nonlinearity\n",
    "def nonlin(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formula of Sigmoid function: $$S(t) = \\frac{1}{(1+e^t)}$$\n",
    "- Sigmoid function is the activation function that maps inputs to any numbesr between 0 to 1, which represents probabilities.\n",
    "- The input, x, for this activation function is the dot product of layer_0 (represent the actual data to be fed in the model) and synapse_0 (gives weights on corresponding features in the layer_0 data).\n",
    "- This tranforms the matrix which contains values between -infinity and infinity into parobablistic values between 0 to 1.\n",
    "- The returned value of this function is the next layer, layer_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The input (output) is the layer_1 matrix\n",
    "- The magic formula is the result of taking derivative of sigmoid function and simplify it:\n",
    "- The process of making the simplified derivative of sigmoid:\n",
    "$$S(x) = \\frac{1}{(1+e^x)}$$\n",
    "$$f(x) = \\frac{1}{S(x)} = 1+e^x$$\n",
    "$$f'(x) = \\frac{S'(x)}{S(x)^2}$$\n",
    "$$f'(x) = -e^{-x} = 1 - f(x) = 1 - \\frac{1}{S(x)}\n",
    "= \\frac{(S(x)-1)}{S(x)}$$\n",
    "$$ \\frac{S'(x)}{S(x)^2} = \\frac{(S(x)-1)}{S(x)}$$ \n",
    "$$ S'(x) = {S(x)^2}\\frac{(S(x)-1)}{S(x)}$$ \n",
    "$$ S'(x) = {S(x)}(S(x)-1)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- every [] represents a raw of table, a sample of a set of samples.\n",
    "- every index of [] represents a column of table, a feature of a set of features.\n",
    "- [0, 1]: a sample contains value 0, for feature 1 and value 1 for feature 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output dataset            \n",
    "y = np.array([[0,1,1,0]]).T\n",
    "# same as y = np.array([[0],\n",
    "#                       [1],\n",
    "#                       [1],\n",
    "#                       [0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make sure the shape of the array is transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.random.random: https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random.html\n",
    "- np.random.random((3,4)) generates 3x4 matrix contains values in each cell between [0, 1)\n",
    "- by 2*np.random.random((3,4)) - 1, the range of values in each cell becomces [-1, 1)\n",
    "- so 2*np.random.random((4,1)) - 1 generates 4x1 matrix contains values in each cell between [-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for iter in range(100000):\n",
    "\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0)) # 4x3 * 3x4 = 4x4\n",
    "    l2 = nonlin(np.dot(l1,syn1)) # 4x4 * 4x1 = 4x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- l0: is layer 0 of the neural nets, which represents the source input of the dataset\n",
    "- l1: is layer 1 of the neural nets, which represents the first hidden layer which is the result of mapping the weights onto layer 0 values through synapse 0 (paths between layer 0 to layer 1).\n",
    "- l2: is layer 3 of the neural nets, which represents the output layer which is the result of mappint the weights onto layer 1 values throug synapse 1 (paths between layer 1 to layer 2).\n",
    "- Hidden layer's all valuse (l1 and l2 in this case) should always be [0, 1) because of application of nonlinear transformation by the activation function, in this exmaple Sigmoid function.\n",
    "- Think of layers as states of pulses at a given time and synapses as paths which gives weights on pulses (either strengthen, weaken, or do nothing)\n",
    "- What we are training through neural nets training is the weights of synapses (how much change it gives pulses passing through the synapse). We would like to find the right configuration of synapses to achieve the ability to give right weights to each states of pulses for target output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # how much did we miss the target value?\n",
    "    l2_error = y - l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By knowing the difference between the output originally given and computed through neural nets, we use this information to how much degree we should adjust the weights of synapses.\n",
    "- This is where the direction of error is computed. l2 is known to be [0, 1) where y ranges 0 or 1. If one of the value of l2 is 0.5 and the corresponding value of y is 0 and then, we know that in the next iterations we should weigh the value down. If y is 1 then, we should weight the value up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    if (iter% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since l2 ranges [0, 1), its derivative (computed by x * (1 - x)) should also range [0, 1)\n",
    "- Notice that the derivative gain maximum value when x is closer to 0.5. This is understandable when we think about the output is supposed to get closer to either 0 or 1 through training process. Theoritically, 0.5 is the furthest value of state for the training preference so we need more adjustment to put in the synapses.\n",
    "- Note again that the l2_error represents the direction that the weights should be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.47372957]\n",
      " [ 0.48895696]\n",
      " [ 0.54384086]\n",
      " [ 0.54470837]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Output After Training:\")\n",
    "print(l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
