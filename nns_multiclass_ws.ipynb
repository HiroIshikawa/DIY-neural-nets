{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n",
      "Error:0.430362961468\n",
      "Error:0.444444444444\n",
      "Error:0.444444444444\n",
      "Error:0.444444444444\n",
      "Error:0.444444444444\n",
      "Error:0.444444444444\n",
      "Error:0.444444444444\n",
      "Error:0.444444444444\n",
      "Error:0.444444444444\n",
      "Error:0.444444444444\n",
      "\n",
      "Output After Training:\n",
      "[[  7.90635305e-123   1.00000000e+000   4.74011931e-201]\n",
      " [  7.91726852e-123   1.00000000e+000   4.76670172e-201]\n",
      " [  7.95650496e-123   1.00000000e+000   4.73776767e-201]\n",
      " [  7.92156158e-123   1.00000000e+000   4.77674997e-201]\n",
      " [  7.91378057e-123   1.00000000e+000   4.73294574e-201]\n",
      " [  7.85364371e-123   1.00000000e+000   4.77059038e-201]\n",
      " [  7.93517601e-123   1.00000000e+000   4.74192378e-201]\n",
      " [  7.89548598e-123   1.00000000e+000   4.75540630e-201]\n",
      " [  7.96838172e-123   1.00000000e+000   4.78737322e-201]\n",
      " [  7.90651775e-123   1.00000000e+000   4.76896186e-201]\n",
      " [  7.87978849e-123   1.00000000e+000   4.74995983e-201]\n",
      " [  7.89103760e-123   1.00000000e+000   4.76713119e-201]\n",
      " [  7.93375486e-123   1.00000000e+000   4.76392827e-201]\n",
      " [  8.09945810e-123   1.00000000e+000   4.72228753e-201]\n",
      " [  7.90203731e-123   1.00000000e+000   4.72771663e-201]\n",
      " [  7.86899366e-123   1.00000000e+000   4.74602409e-201]\n",
      " [  7.89674085e-123   1.00000000e+000   4.73054783e-201]\n",
      " [  7.89979177e-123   1.00000000e+000   4.74409812e-201]\n",
      " [  7.84917173e-123   1.00000000e+000   4.77324642e-201]\n",
      " [  7.88916895e-123   1.00000000e+000   4.74293453e-201]\n",
      " [  7.85545655e-123   1.00000000e+000   4.77961620e-201]\n",
      " [  7.88229317e-123   1.00000000e+000   4.75179920e-201]\n",
      " [  8.06127194e-123   1.00000000e+000   4.65620204e-201]\n",
      " [  7.84954329e-123   1.00000000e+000   4.80064153e-201]\n",
      " [  7.85118752e-123   1.00000000e+000   4.80652607e-201]\n",
      " [  7.87845281e-123   1.00000000e+000   4.79080542e-201]\n",
      " [  7.87020101e-123   1.00000000e+000   4.77752302e-201]\n",
      " [  7.88623742e-123   1.00000000e+000   4.75348189e-201]\n",
      " [  7.89947435e-123   1.00000000e+000   4.74673489e-201]\n",
      " [  7.89567999e-123   1.00000000e+000   4.78160932e-201]\n",
      " [  7.88940956e-123   1.00000000e+000   4.78732627e-201]\n",
      " [  7.86620962e-123   1.00000000e+000   4.76834779e-201]\n",
      " [  7.89943228e-123   1.00000000e+000   4.72675938e-201]\n",
      " [  7.89401165e-123   1.00000000e+000   4.72862049e-201]\n",
      " [  7.90651775e-123   1.00000000e+000   4.76896186e-201]\n",
      " [  7.94994627e-123   1.00000000e+000   4.72821400e-201]\n",
      " [  7.89851877e-123   1.00000000e+000   4.73967041e-201]\n",
      " [  7.90651775e-123   1.00000000e+000   4.76896186e-201]\n",
      " [  7.99502159e-123   1.00000000e+000   4.75862736e-201]\n",
      " [  7.89013909e-123   1.00000000e+000   4.75655944e-201]\n",
      " [  7.92331367e-123   1.00000000e+000   4.72900146e-201]\n",
      " [  7.99550010e-123   1.00000000e+000   4.85260300e-201]\n",
      " [  7.99225757e-123   1.00000000e+000   4.73734839e-201]\n",
      " [  7.86113523e-123   1.00000000e+000   4.78444320e-201]\n",
      " [  7.84211427e-123   1.00000000e+000   4.79361912e-201]\n",
      " [  7.91721577e-123   1.00000000e+000   4.77166910e-201]\n",
      " [  7.88126436e-123   1.00000000e+000   4.75007339e-201]\n",
      " [  7.94302015e-123   1.00000000e+000   4.75289932e-201]\n",
      " [  7.88401492e-123   1.00000000e+000   4.74764289e-201]\n",
      " [  7.91100771e-123   1.00000000e+000   4.74837757e-201]\n",
      " [  7.78460443e-123   1.00000000e+000   5.12974447e-201]\n",
      " [  7.77211126e-123   1.00000000e+000   5.25279604e-201]\n",
      " [  7.77810993e-123   1.00000000e+000   5.23031286e-201]\n",
      " [  7.74167892e-123   1.00000000e+000   5.45419996e-201]\n",
      " [  7.76781227e-123   1.00000000e+000   5.30934607e-201]\n",
      " [  7.75159516e-123   1.00000000e+000   5.44266766e-201]\n",
      " [  7.76674397e-123   1.00000000e+000   5.35021212e-201]\n",
      " [  7.74617313e-123   1.00000000e+000   5.29648620e-201]\n",
      " [  7.77603504e-123   1.00000000e+000   5.20109179e-201]\n",
      " [  7.73773358e-123   1.00000000e+000   5.49142235e-201]\n",
      " [  7.73930296e-123   1.00000000e+000   5.40169526e-201]\n",
      " [  7.75889088e-123   1.00000000e+000   5.34498253e-201]\n",
      " [  7.76299934e-123   1.00000000e+000   5.21346468e-201]\n",
      " [  7.76032553e-123   1.00000000e+000   5.39606712e-201]\n",
      " [  7.76036179e-123   1.00000000e+000   5.21673589e-201]\n",
      " [  7.78017938e-123   1.00000000e+000   5.14736489e-201]\n",
      " [  7.74634894e-123   1.00000000e+000   5.53288299e-201]\n",
      " [  7.76421115e-123   1.00000000e+000   5.21377595e-201]\n",
      " [  7.74998692e-123   1.00000000e+000   5.48028793e-201]\n",
      " [  7.75534313e-123   1.00000000e+000   5.28021837e-201]\n",
      " [  7.74645125e-123   1.00000000e+000   5.63445698e-201]\n",
      " [  7.76802949e-123   1.00000000e+000   5.20395095e-201]\n",
      " [  7.75419831e-123   1.00000000e+000   5.50953801e-201]\n",
      " [  7.76355947e-123   1.00000000e+000   5.33105016e-201]\n",
      " [  7.77410609e-123   1.00000000e+000   5.18592488e-201]\n",
      " [  7.77691201e-123   1.00000000e+000   5.17979214e-201]\n",
      " [  7.77529733e-123   1.00000000e+000   5.24105494e-201]\n",
      " [  7.76648324e-123   1.00000000e+000   5.40176203e-201]\n",
      " [  7.75691410e-123   1.00000000e+000   5.41498092e-201]\n",
      " [  7.76655102e-123   1.00000000e+000   5.12577304e-201]\n",
      " [  7.75187393e-123   1.00000000e+000   5.29794239e-201]\n",
      " [  7.75565759e-123   1.00000000e+000   5.23880011e-201]\n",
      " [  7.76190026e-123   1.00000000e+000   5.23374869e-201]\n",
      " [  7.74271106e-123   1.00000000e+000   5.70462996e-201]\n",
      " [  7.73964803e-123   1.00000000e+000   5.60900294e-201]\n",
      " [  7.76254173e-123   1.00000000e+000   5.36954081e-201]\n",
      " [  7.77570074e-123   1.00000000e+000   5.23649688e-201]\n",
      " [  7.76171703e-123   1.00000000e+000   5.31810517e-201]\n",
      " [  7.75663366e-123   1.00000000e+000   5.32572508e-201]\n",
      " [  7.74555028e-123   1.00000000e+000   5.41569231e-201]\n",
      " [  7.74534192e-123   1.00000000e+000   5.47398476e-201]\n",
      " [  7.76300270e-123   1.00000000e+000   5.35190189e-201]\n",
      " [  7.75922718e-123   1.00000000e+000   5.27193894e-201]\n",
      " [  7.74719862e-123   1.00000000e+000   5.28578021e-201]\n",
      " [  7.75015022e-123   1.00000000e+000   5.40563766e-201]\n",
      " [  7.76084776e-123   1.00000000e+000   5.28611954e-201]\n",
      " [  7.75676611e-123   1.00000000e+000   5.34011295e-201]\n",
      " [  7.76937833e-123   1.00000000e+000   5.23179080e-201]\n",
      " [  7.75539777e-123   1.00000000e+000   5.18248619e-201]\n",
      " [  7.75611374e-123   1.00000000e+000   5.33013016e-201]\n",
      " [  7.71739119e-123   1.00000000e+000   6.38564517e-201]\n",
      " [  7.72452118e-123   1.00000000e+000   5.98493342e-201]\n",
      " [  7.75311942e-123   1.00000000e+000   5.74778334e-201]\n",
      " [  7.74227381e-123   1.00000000e+000   5.83303174e-201]\n",
      " [  7.73287025e-123   1.00000000e+000   6.05872567e-201]\n",
      " [  7.75540203e-123   1.00000000e+000   5.78453502e-201]\n",
      " [  7.70221450e-123   1.00000000e+000   6.09367987e-201]\n",
      " [  7.76021446e-123   1.00000000e+000   5.65242554e-201]\n",
      " [  7.74325168e-123   1.00000000e+000   5.82980730e-201]\n",
      " [  7.74894695e-123   1.00000000e+000   5.87743751e-201]\n",
      " [  7.75409928e-123   1.00000000e+000   5.61010801e-201]\n",
      " [  7.74249304e-123   1.00000000e+000   5.78430536e-201]\n",
      " [  7.75047636e-123   1.00000000e+000   5.73113251e-201]\n",
      " [  7.71205933e-123   1.00000000e+000   6.11509774e-201]\n",
      " [  7.70500418e-123   1.00000000e+000   6.30559947e-201]\n",
      " [  7.73740273e-123   1.00000000e+000   5.90940157e-201]\n",
      " [  7.75179492e-123   1.00000000e+000   5.68280188e-201]\n",
      " [  7.76468735e-123   1.00000000e+000   5.64288285e-201]\n",
      " [  7.73626828e-123   1.00000000e+000   6.10080715e-201]\n",
      " [  7.73567643e-123   1.00000000e+000   5.72990993e-201]\n",
      " [  7.74693540e-123   1.00000000e+000   5.83576046e-201]\n",
      " [  7.71837147e-123   1.00000000e+000   6.03604904e-201]\n",
      " [  7.75627195e-123   1.00000000e+000   5.76510224e-201]\n",
      " [  7.74820141e-123   1.00000000e+000   5.62218424e-201]\n",
      " [  7.74990372e-123   1.00000000e+000   5.77570172e-201]\n",
      " [  7.76644892e-123   1.00000000e+000   5.52872904e-201]\n",
      " [  7.74821762e-123   1.00000000e+000   5.60490555e-201]\n",
      " [  7.74769786e-123   1.00000000e+000   5.63475401e-201]\n",
      " [  7.73236431e-123   1.00000000e+000   6.01000809e-201]\n",
      " [  7.77157020e-123   1.00000000e+000   5.41476974e-201]\n",
      " [  7.76127118e-123   1.00000000e+000   5.62324896e-201]\n",
      " [  7.77830002e-123   1.00000000e+000   5.38329394e-201]\n",
      " [  7.72834786e-123   1.00000000e+000   6.07935711e-201]\n",
      " [  7.75698421e-123   1.00000000e+000   5.51096703e-201]\n",
      " [  7.74191200e-123   1.00000000e+000   5.75264323e-201]\n",
      " [  7.75942070e-123   1.00000000e+000   5.68086466e-201]\n",
      " [  7.72981994e-123   1.00000000e+000   6.10013148e-201]\n",
      " [  7.75058460e-123   1.00000000e+000   5.70126241e-201]\n",
      " [  7.74577517e-123   1.00000000e+000   5.63956778e-201]\n",
      " [  7.75639952e-123   1.00000000e+000   5.63073710e-201]\n",
      " [  7.73669137e-123   1.00000000e+000   5.98316880e-201]\n",
      " [  7.75346584e-123   1.00000000e+000   5.63832629e-201]\n",
      " [  7.72452118e-123   1.00000000e+000   5.98493342e-201]\n",
      " [  7.74084602e-123   1.00000000e+000   5.96406297e-201]\n",
      " [  7.73516774e-123   1.00000000e+000   6.03898798e-201]\n",
      " [  7.74431382e-123   1.00000000e+000   5.78298453e-201]\n",
      " [  7.73910333e-123   1.00000000e+000   5.76276916e-201]\n",
      " [  7.74920536e-123   1.00000000e+000   5.69062728e-201]\n",
      " [  7.73349255e-123   1.00000000e+000   5.99404851e-201]\n",
      " [  7.73855668e-123   1.00000000e+000   5.79402394e-201]]\n",
      "This is success!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n# input dataset\\n# X = np.array([ [0,1],\\n#                [0,1],\\n#                [1,0],\\n#                [1,0] ])\\n\\n# output dataset\\n# y = np.array([ [0],\\n#                [0],\\n#                [1],\\n#                [1] ])\\n# or simplify\\n# y = np.array([[0,0,1,1]]).T\\n\\n# seed random numbers to make calculation\\n# this is neccessary to observe the result under same random numbers in one session\\nnp.random.seed(1)\\n\\n# hidden layer configuration.\\n# 3 layear: You need to configure synapse 0 and synapse 1 here.\\nsynapse_0 = 2* np.random.random((2, 4)) - 1 # 2x4\\nsynapse_1 = 2* np.random.random((4, 1)) - 1 # 4x1\\n\\n# start training iterations (10k times)\\nfor i in range(100000):\\n    # iteration start\\n    num_iter = i\\n    \\n    # ---- training iteration step 1: Forward Propagation\\n    # Forward Propagation applies weights of synapses in each layer to produce output layer\\n    # Output layer is the layer that is the last layer as a final output for an iteration of training\\n    # the initial layer is the input\\n    l0 = X # 4x2\\n    # the hidden layer after application of the first synapse weights to layer 0 values, which is the input\\n    l0_weighted = np.dot(l0,synapse_0) # 4x2 2x4 -> 4x4\\n    # smooth each weighted value into new value ranging [0, 1)\\n    # now we generated the hidden layer, l1\\n    l1 = nonlin(l0_weighted) # -> 4x4\\n    \\n    # we finally compute the output layer by applying the second synapse weights to layer 1 values\\n    l1_weighted = np.dot(l1,synapse_1) # 4x4 4x1 -> 4x1\\n    # smooth each weighted value into new value ranging [0, 1)\\n    # now we generated the hidden layer, l1\\n    l2 = nonlin(l1_weighted) # -> 4x1\\n    # ---- Forward Propagation completed\\n    \\n    \\n    # ---- training iteration step 2: Back Propagation\\n    # Back Propagation adjust each weight of synapse layer by layer\\n    \\n    # First we figure out the initial error amount and direction on the output layer relative to the given output y\\n    l2_error = y - l2 # 4x1 - 4x1 -> 4x1\\n    # Incorporate the derivative of sigmoid on layer 2 (output layer)\\n    l2_delta = l2_error * drv_nonlin(l2) # 4x1 4x1 -> 4x1\\n    # Apply the delta computed to each synapses for the next iteration of training\\n    synapse_1 += l1.T.dot(l2_delta) # 4x4.T(=4x4) 4x1 -> 4x1\\n    \\n    # How much weights of synapse 1 contributed to the l2_delta\\n    l1_error = l2_delta.dot(synapse_1.T) # 4x1 4x1.T(=1x4) -> 4x4 \\n    # Incorporate the derivative of sigmoid on layer 1 (hidden layer)\\n    l1_delta = l1_error * drv_nonlin(l1) # 4x4 4x4 -> 4x4    \\n    # Apply the delta computed to each synapses for the next iteration of training\\n    synapse_0 += l0.T.dot(l1_delta) # 4x2.T(=2x4) 4x4 -> 2x4 \\n    # ---- Back Propagation completed\\n    \\n    # Checking error of the output layer respects to given output for each 10000 iterations\\n    # The number should decrease over the iterations\\n    if (num_iter% 10000) == 0:\\n        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\\n        \\n\\nprint(\\'\\')\\nprint(\"Output After Training:\")\\nprint(l2)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# sigmoid for binary problem\n",
    "def nonlin(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def drv_nonlin(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "dataframe = pandas.read_csv(\"data/iris.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "y = dummy_y\n",
    "print(y.shape)\n",
    "\n",
    "# print(X.shape)\n",
    "# print(encoded_Y)\n",
    "# print(encoded_Y.shape)\n",
    "# print(dummy_y.shape)\n",
    "# y = np.array(encoded_Y).reshape(150,1)\n",
    "# print(y.shape)\n",
    "# print(y)\n",
    "\n",
    "# print(X)\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# this is neccessary to observe the result under same random numbers in one session\n",
    "np.random.seed(1)\n",
    "\n",
    "# hidden layer configuration.\n",
    "# 3 layear: You need to configure synapse 0 and synapse 1 here.\n",
    "synapse_0 = 2* np.random.random((X.shape[1], X.shape[0])) - 1 # 3x150\n",
    "synapse_1 = 2* np.random.random((y.shape[0], y.shape[1])) - 1 # 150x1\n",
    "\n",
    "# start training iterations (10k times)\n",
    "for i in range(100000):\n",
    "    # iteration start\n",
    "    num_iter = i\n",
    "    \n",
    "    # ---- training iteration step 1: Forward Propagation\n",
    "    # Forward Propagation applies weights of synapses in each layer to produce output layer\n",
    "    # Output layer is the layer that is the last layer as a final output for an iteration of training\n",
    "    # the initial layer is the input\n",
    "    l0 = X # 4x2\n",
    "    # the hidden layer after application of the first synapse weights to layer 0 values, which is the input\n",
    "    l0_weighted = np.dot(l0,synapse_0) # 4x2 2x4 -> 4x4\n",
    "    # smooth each weighted value into new value ranging [0, 1)\n",
    "    # now we generated the hidden layer, l1\n",
    "    l1 = nonlin(l0_weighted) # -> 4x4\n",
    "    \n",
    "    # we finally compute the output layer by applying the second synapse weights to layer 1 values\n",
    "    l1_weighted = np.dot(l1,synapse_1) # 4x4 4x1 -> 4x1\n",
    "    # smooth each weighted value into new value ranging [0, 1)\n",
    "    # now we generated the hidden layer, l1\n",
    "    l2 = nonlin(l1_weighted) # -> 4x1\n",
    "    # ---- Forward Propagation completed\n",
    "    \n",
    "    \n",
    "    # ---- training iteration step 2: Back Propagation\n",
    "    # Back Propagation adjust each weight of synapse layer by layer\n",
    "    \n",
    "    # First we figure out the initial error amount and direction on the output layer relative to the given output y\n",
    "    l2_error = y - l2 # 4x1 - 4x1 -> 4x1\n",
    "    # Incorporate the derivative of sigmoid on layer 2 (output layer)\n",
    "    l2_delta = l2_error * drv_nonlin(l2) # 4x1 4x1 -> 4x1\n",
    "    # Apply the delta computed to each synapses for the next iteration of training\n",
    "    synapse_1 += l1.T.dot(l2_delta) # 4x4.T(=4x4) 4x1 -> 4x1\n",
    "    \n",
    "    # How much weights of synapse 1 contributed to the l2_delta\n",
    "    l1_error = l2_delta.dot(synapse_1.T) # 4x1 4x1.T(=1x4) -> 4x4 \n",
    "    # Incorporate the derivative of sigmoid on layer 1 (hidden layer)\n",
    "    l1_delta = l1_error * drv_nonlin(l1) # 4x4 4x4 -> 4x4    \n",
    "    # Apply the delta computed to each synapses for the next iteration of training\n",
    "    synapse_0 += l0.T.dot(l1_delta) # 4x2.T(=2x4) 4x4 -> 2x4 \n",
    "    # ---- Back Propagation completed\n",
    "    \n",
    "    # Checking error of the output layer respects to given output for each 10000 iterations\n",
    "    # The number should decrease over the iterations\n",
    "    if (num_iter% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "\n",
    "print('')\n",
    "print(\"Output After Training:\")\n",
    "print(l2)\n",
    "\n",
    "\n",
    "print(\"This is success!\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# input dataset\n",
    "# X = np.array([ [0,1],\n",
    "#                [0,1],\n",
    "#                [1,0],\n",
    "#                [1,0] ])\n",
    "\n",
    "# output dataset\n",
    "# y = np.array([ [0],\n",
    "#                [0],\n",
    "#                [1],\n",
    "#                [1] ])\n",
    "# or simplify\n",
    "# y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# this is neccessary to observe the result under same random numbers in one session\n",
    "np.random.seed(1)\n",
    "\n",
    "# hidden layer configuration.\n",
    "# 3 layear: You need to configure synapse 0 and synapse 1 here.\n",
    "synapse_0 = 2* np.random.random((2, 4)) - 1 # 2x4\n",
    "synapse_1 = 2* np.random.random((4, 1)) - 1 # 4x1\n",
    "\n",
    "# start training iterations (10k times)\n",
    "for i in range(100000):\n",
    "    # iteration start\n",
    "    num_iter = i\n",
    "    \n",
    "    # ---- training iteration step 1: Forward Propagation\n",
    "    # Forward Propagation applies weights of synapses in each layer to produce output layer\n",
    "    # Output layer is the layer that is the last layer as a final output for an iteration of training\n",
    "    # the initial layer is the input\n",
    "    l0 = X # 4x2\n",
    "    # the hidden layer after application of the first synapse weights to layer 0 values, which is the input\n",
    "    l0_weighted = np.dot(l0,synapse_0) # 4x2 2x4 -> 4x4\n",
    "    # smooth each weighted value into new value ranging [0, 1)\n",
    "    # now we generated the hidden layer, l1\n",
    "    l1 = nonlin(l0_weighted) # -> 4x4\n",
    "    \n",
    "    # we finally compute the output layer by applying the second synapse weights to layer 1 values\n",
    "    l1_weighted = np.dot(l1,synapse_1) # 4x4 4x1 -> 4x1\n",
    "    # smooth each weighted value into new value ranging [0, 1)\n",
    "    # now we generated the hidden layer, l1\n",
    "    l2 = nonlin(l1_weighted) # -> 4x1\n",
    "    # ---- Forward Propagation completed\n",
    "    \n",
    "    \n",
    "    # ---- training iteration step 2: Back Propagation\n",
    "    # Back Propagation adjust each weight of synapse layer by layer\n",
    "    \n",
    "    # First we figure out the initial error amount and direction on the output layer relative to the given output y\n",
    "    l2_error = y - l2 # 4x1 - 4x1 -> 4x1\n",
    "    # Incorporate the derivative of sigmoid on layer 2 (output layer)\n",
    "    l2_delta = l2_error * drv_nonlin(l2) # 4x1 4x1 -> 4x1\n",
    "    # Apply the delta computed to each synapses for the next iteration of training\n",
    "    synapse_1 += l1.T.dot(l2_delta) # 4x4.T(=4x4) 4x1 -> 4x1\n",
    "    \n",
    "    # How much weights of synapse 1 contributed to the l2_delta\n",
    "    l1_error = l2_delta.dot(synapse_1.T) # 4x1 4x1.T(=1x4) -> 4x4 \n",
    "    # Incorporate the derivative of sigmoid on layer 1 (hidden layer)\n",
    "    l1_delta = l1_error * drv_nonlin(l1) # 4x4 4x4 -> 4x4    \n",
    "    # Apply the delta computed to each synapses for the next iteration of training\n",
    "    synapse_0 += l0.T.dot(l1_delta) # 4x2.T(=2x4) 4x4 -> 2x4 \n",
    "    # ---- Back Propagation completed\n",
    "    \n",
    "    # Checking error of the output layer respects to given output for each 10000 iterations\n",
    "    # The number should decrease over the iterations\n",
    "    if (num_iter% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "\n",
    "print('')\n",
    "print(\"Output After Training:\")\n",
    "print(l2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
