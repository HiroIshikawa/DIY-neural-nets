{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training With Alpha:1e-05\n",
      "Error:0.499754731319\n",
      "Error:0.499353445001\n",
      "Error:0.4989541838\n",
      "Error:0.498556842017\n",
      "Error:0.498161315246\n",
      "Error:0.497767500353\n",
      "Error:0.497375295452\n",
      "Error:0.496984599882\n",
      "Error:0.496595314183\n",
      "Error:0.496207340072\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.51697892]\n",
      " [ 0.51697892]\n",
      " [ 0.52533768]\n",
      " [ 0.52533768]]\n",
      "\n",
      "Training With Alpha:0.0001\n",
      "Error:0.499754731319\n",
      "Error:0.495820572435\n",
      "Error:0.49199917054\n",
      "Error:0.4882000124\n",
      "Error:0.48434135284\n",
      "Error:0.480348227251\n",
      "Error:0.476150878647\n",
      "Error:0.471683627966\n",
      "Error:0.46688418429\n",
      "Error:0.461693389992\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.50717481]\n",
      " [ 0.50717481]\n",
      " [ 0.59506281]\n",
      " [ 0.59506281]]\n",
      "\n",
      "Training With Alpha:0.001\n",
      "Error:0.499754731319\n",
      "Error:0.456054480836\n",
      "Error:0.367967991778\n",
      "Error:0.254029741167\n",
      "Error:0.180756858637\n",
      "Error:0.140467121646\n",
      "Error:0.116275526475\n",
      "Error:0.100279438755\n",
      "Error:0.0889014906537\n",
      "Error:0.0803625936956\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.0754142 ]\n",
      " [ 0.0754142 ]\n",
      " [ 0.92802939]\n",
      " [ 0.92802939]]\n",
      "\n",
      "Training With Alpha:0.003\n",
      "Error:0.499754731319\n",
      "Error:0.254019135062\n",
      "Error:0.116271702835\n",
      "Error:0.0803611624564\n",
      "Error:0.0638792959112\n",
      "Error:0.054176442176\n",
      "Error:0.0476707788645\n",
      "Error:0.0429483478784\n",
      "Error:0.0393324353593\n",
      "Error:0.0364557167384\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.03475617]\n",
      " [ 0.03475617]\n",
      " [ 0.96655522]\n",
      " [ 0.96655522]]\n",
      "\n",
      "Training With Alpha:0.01\n",
      "Error:0.499754731319\n",
      "Error:0.0736870217019\n",
      "Error:0.0443752096661\n",
      "Error:0.0341013312838\n",
      "Error:0.0285503060596\n",
      "Error:0.0249722534056\n",
      "Error:0.0224301935445\n",
      "Error:0.020508820431\n",
      "Error:0.0189929155028\n",
      "Error:0.0177586319938\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.01704446]\n",
      " [ 0.01704446]\n",
      " [ 0.98358604]\n",
      " [ 0.98358604]]\n",
      "\n",
      "Training With Alpha:0.03\n",
      "Error:0.499754731319\n",
      "Error:0.0341043868934\n",
      "Error:0.0224340753375\n",
      "Error:0.017762382271\n",
      "Error:0.0151017491604\n",
      "Error:0.0133356195354\n",
      "Error:0.0120566903063\n",
      "Error:0.0110768831079\n",
      "Error:0.0102959697042\n",
      "Error:0.0096550768495\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.00929297]\n",
      " [ 0.00929297]\n",
      " [ 0.99105869]\n",
      " [ 0.99105869]]\n",
      "\n",
      "Training With Alpha:0.1\n",
      "Error:0.499754731319\n",
      "Error:0.0167456722771\n",
      "Error:0.0113884319309\n",
      "Error:0.00912665702484\n",
      "Error:0.00780975343042\n",
      "Error:0.00692451351742\n",
      "Error:0.00627814564564\n",
      "Error:0.00578001753334\n",
      "Error:0.00538123139874\n",
      "Error:0.00505280193253\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.00487123]\n",
      " [ 0.00487123]\n",
      " [ 0.99531853]\n",
      " [ 0.99531853]]\n",
      "\n",
      "Training With Alpha:0.3\n",
      "Error:0.499754731319\n",
      "Error:0.00915402546784\n",
      "Error:0.00629961161805\n",
      "Error:0.00507117663361\n",
      "Error:0.00435005898863\n",
      "Error:0.00386302520141\n",
      "Error:0.0035063023775\n",
      "Error:0.00323077660066\n",
      "Error:0.00300982604851\n",
      "Error:0.00282761523959\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.00272821]\n",
      " [ 0.00272821]\n",
      " [ 0.99738006]\n",
      " [ 0.99738006]]\n",
      "\n",
      "Training With Alpha:1\n",
      "Error:0.499754731319\n",
      "Error:0.00485688337934\n",
      "Error:0.00336336381133\n",
      "Error:0.00271439021378\n",
      "Error:0.00233179194978\n",
      "Error:0.00207273083362\n",
      "Error:0.00188265631968\n",
      "Error:0.00173566067165\n",
      "Error:0.00161766639497\n",
      "Error:0.00152028432769\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.00146799]\n",
      " [ 0.00146799]\n",
      " [ 0.99859164]\n",
      " [ 0.99859164]]\n",
      "\n",
      "Training With Alpha:3\n",
      "Error:0.499754731319\n",
      "Error:0.00283752511297\n",
      "Error:0.00197770397089\n",
      "Error:0.001601264517\n",
      "Error:0.00137842920566\n",
      "Error:0.00122712658277\n",
      "Error:0.0011158841223\n",
      "Error:0.00102971170287\n",
      "Error:0.000960446273027\n",
      "Error:0.000903214632568\n",
      "\n",
      "Output After Training:\n",
      "[[  8.73785571e-04]\n",
      " [  8.73785571e-04]\n",
      " [  9.99163966e-01]\n",
      " [  9.99163966e-01]]\n",
      "\n",
      "Training With Alpha:10\n",
      "Error:0.499754731319\n",
      "Error:0.00152288249106\n",
      "Error:0.00106062132494\n",
      "Error:0.000858124864207\n",
      "Error:0.000738270927933\n",
      "Error:0.000656913661611\n",
      "Error:0.000597115675117\n",
      "Error:0.000550808346178\n",
      "Error:0.000513597632367\n",
      "Error:0.00048286045523\n",
      "\n",
      "Output After Training:\n",
      "[[  4.58467401e-04]\n",
      " [  4.58467401e-04]\n",
      " [  9.99544619e-01]\n",
      " [  9.99544619e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid for binary problem\n",
    "def nonlin(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def drv_nonlin(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "# input dataset\n",
    "X = np.array([ [0,1],\n",
    "               [0,1],\n",
    "               [1,0],\n",
    "               [1,0] ])\n",
    "\n",
    "# output dataset\n",
    "# y = np.array([ [0],\n",
    "#                [0],\n",
    "#                [1],\n",
    "#                [1] ])\n",
    "# or simplify\n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "alphas = [0.00001, 0.0001, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(\"\\nTraining With Alpha:\" + str(alpha))\n",
    "\n",
    "    # seed random numbers to make calculation\n",
    "    # this is neccessary to observe the result under same random numbers in one session\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # hidden layer configuration.\n",
    "    # 3 layear: You need to configure synapse 0 and synapse 1 here.\n",
    "    synapse_0 = 2* np.random.random((2, 4)) - 1 # 2x4\n",
    "    synapse_1 = 2* np.random.random((4, 1)) - 1 # 4x1\n",
    "\n",
    "    # start training iterations (10k times)\n",
    "    for i in range(100000):\n",
    "        # iteration start\n",
    "        num_iter = i\n",
    "\n",
    "        # ---- training iteration step 1: Forward Propagation\n",
    "        # Forward Propagation applies weights of synapses in each layer to produce output layer\n",
    "        # Output layer is the layer that is the last layer as a final output for an iteration of training\n",
    "        # the initial layer is the input\n",
    "        l0 = X # 4x2\n",
    "        # the hidden layer after application of the first synapse weights to layer 0 values, which is the input\n",
    "        l0_weighted = np.dot(l0,synapse_0) # 4x2 2x4 -> 4x4\n",
    "        # smooth each weighted value into new value ranging [0, 1)\n",
    "        # now we generated the hidden layer, l1\n",
    "        l1 = nonlin(l0_weighted) # -> 4x4\n",
    "\n",
    "        # we finally compute the output layer by applying the second synapse weights to layer 1 values\n",
    "        l1_weighted = np.dot(l1,synapse_1) # 4x4 4x1 -> 4x1\n",
    "        # smooth each weighted value into new value ranging [0, 1)\n",
    "        # now we generated the hidden layer, l1\n",
    "        l2 = nonlin(l1_weighted) # -> 4x1\n",
    "        # ---- Forward Propagation completed\n",
    "\n",
    "\n",
    "        # ---- training iteration step 2: Back Propagation\n",
    "        # Back Propagation adjust each weight of synapse layer by layer\n",
    "\n",
    "        # First we figure out the initial error amount and direction on the output layer relative to the given output y\n",
    "        l2_error = y - l2 # 4x1 - 4x1 -> 4x1\n",
    "        # Incorporate the derivative of sigmoid on layer 2 (output layer)\n",
    "        l2_delta = l2_error * drv_nonlin(l2) # 4x1 4x1 -> 4x1\n",
    "        # Apply the delta computed to each synapses for the next iteration of training\n",
    "        synapse_1 += alpha*l1.T.dot(l2_delta) # 4x4.T(=4x4) 4x1 -> 4x1\n",
    "\n",
    "        # How much weights of synapse 1 contributed to the l2_delta\n",
    "        l1_error = l2_delta.dot(synapse_1.T) # 4x1 4x1.T(=1x4) -> 4x4 \n",
    "        # Incorporate the derivative of sigmoid on layer 1 (hidden layer)\n",
    "        l1_delta = l1_error * drv_nonlin(l1) # 4x4 4x4 -> 4x4    \n",
    "        # Apply the delta computed to each synapses for the next iteration of training\n",
    "        synapse_0 += alpha*l0.T.dot(l1_delta) # 4x2.T(=2x4) 4x4 -> 2x4 \n",
    "        # ---- Back Propagation completed\n",
    "\n",
    "        # Checking error of the output layer respects to given output for each 10000 iterations\n",
    "        # The number should decrease over the iterations\n",
    "        if (num_iter% 10000) == 0:\n",
    "            print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "\n",
    "\n",
    "    print('')\n",
    "    print(\"Output After Training:\")\n",
    "    print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
