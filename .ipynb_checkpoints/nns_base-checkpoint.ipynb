{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base 3-Layer Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.499754731319\n",
      "Error:0.00485688337934\n",
      "Error:0.00336336381133\n",
      "Error:0.00271439021378\n",
      "Error:0.00233179194978\n",
      "Error:0.00207273083362\n",
      "Error:0.00188265631968\n",
      "Error:0.00173566067165\n",
      "Error:0.00161766639497\n",
      "Error:0.00152028432769\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.00146799]\n",
      " [ 0.00146799]\n",
      " [ 0.99859164]\n",
      " [ 0.99859164]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid for binary problem\n",
    "def nonlin(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def drv_nonlin(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "# input dataset\n",
    "X = np.array([ [0,1],\n",
    "               [0,1],\n",
    "               [1,0],\n",
    "               [1,0] ])\n",
    "\n",
    "# output dataset\n",
    "# y = np.array([ [0],\n",
    "#                [0],\n",
    "#                [1],\n",
    "#                [1] ])\n",
    "# or simplify\n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# this is neccessary to observe the result under same random numbers in one session\n",
    "np.random.seed(1)\n",
    "\n",
    "# hidden layer configuration.\n",
    "# 3 layear: You need to configure synapse 0 and synapse 1 here.\n",
    "synapse_0 = 2* np.random.random((2, 4)) - 1 # 2x4\n",
    "synapse_1 = 2* np.random.random((4, 1)) - 1 # 4x1\n",
    "\n",
    "# start training iterations (10k times)\n",
    "for i in range(100000):\n",
    "    # iteration start\n",
    "    num_iter = i\n",
    "    \n",
    "    # ---- training iteration step 1: Forward Propagation\n",
    "    # Forward Propagation applies weights of synapses in each layer to produce output layer\n",
    "    # Output layer is the layer that is the last layer as a final output for an iteration of training\n",
    "    # the initial layer is the input\n",
    "    l0 = X # 4x2\n",
    "    # the hidden layer after application of the first synapse weights to layer 0 values, which is the input\n",
    "    l0_weighted = np.dot(l0,synapse_0) # 4x2 2x4 -> 4x4\n",
    "    # smooth each weighted value into new value ranging [0, 1)\n",
    "    # now we generated the hidden layer, l1\n",
    "    l1 = nonlin(l0_weighted) # -> 4x4\n",
    "    \n",
    "    # we finally compute the output layer by applying the second synapse weights to layer 1 values\n",
    "    l1_weighted = np.dot(l1,synapse_1) # 4x4 4x1 -> 4x1\n",
    "    # smooth each weighted value into new value ranging [0, 1)\n",
    "    # now we generated the hidden layer, l1\n",
    "    l2 = nonlin(l1_weighted) # -> 4x1\n",
    "    # ---- Forward Propagation completed\n",
    "    \n",
    "    \n",
    "    # ---- training iteration step 2: Back Propagation\n",
    "    # Back Propagation adjust each weight of synapse layer by layer\n",
    "    \n",
    "    # First we figure out the initial error amount and direction on the output layer relative to the given output y\n",
    "    l2_error = y - l2 # 4x1 - 4x1 -> 4x1\n",
    "    # Incorporate the derivative of sigmoid on layer 2 (output layer)\n",
    "    l2_delta = l2_error * drv_nonlin(l2) # 4x1 4x1 -> 4x1\n",
    "    # Apply the delta computed to each synapses for the next iteration of training\n",
    "    synapse_1 += l1.T.dot(l2_delta) # 4x4.T(=4x4) 4x1 -> 4x1\n",
    "    \n",
    "    # How much weights of synapse 1 contributed to the l2_delta\n",
    "    l1_error = l2_delta.dot(synapse_1.T) # 4x1 4x1.T(=1x4) -> 4x4 \n",
    "    # Incorporate the derivative of sigmoid on layer 1 (hidden layer)\n",
    "    l1_delta = l1_error * drv_nonlin(l1) # 4x4 4x4 -> 4x4    \n",
    "    # Apply the delta computed to each synapses for the next iteration of training\n",
    "    synapse_0 += l0.T.dot(l1_delta) # 4x2.T(=2x4) 4x4 -> 2x4 \n",
    "    # ---- Back Propagation completed\n",
    "    \n",
    "    # Checking error of the output layer respects to given output for each 10000 iterations\n",
    "    # The number should decrease over the iterations\n",
    "    if (num_iter% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "\n",
    "print('')\n",
    "print(\"Output After Training:\")\n",
    "print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
