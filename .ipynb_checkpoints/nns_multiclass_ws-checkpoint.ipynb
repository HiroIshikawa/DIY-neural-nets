{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (150,150) and (3,150) not aligned: 150 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-e9497ab59752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# we finally compute the output layer by applying the second synapse weights to layer 1 values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0ml1_weighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msynapse_1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 4x4 4x1 -> 4x1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;31m# smooth each weighted value into new value ranging [0, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# now we generated the hidden layer, l1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (150,150) and (3,150) not aligned: 150 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# sigmoid for binary problem\n",
    "def nonlin(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def drv_nonlin(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "dataframe = pandas.read_csv(\"data/iris.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "y = dummy_y\n",
    "print(y.shape)\n",
    "\n",
    "# print(X.shape)\n",
    "# print(encoded_Y)\n",
    "# print(encoded_Y.shape)\n",
    "# print(dummy_y.shape)\n",
    "# y = np.array(encoded_Y).reshape(150,1)\n",
    "# print(y.shape)\n",
    "# print(y)\n",
    "\n",
    "# print(X)\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# this is neccessary to observe the result under same random numbers in one session\n",
    "np.random.seed(1)\n",
    "\n",
    "# hidden layer configuration.\n",
    "# 3 layear: You need to configure synapse 0 and synapse 1 here.\n",
    "synapse_0 = 2* np.random.random((X.shape[1], X.shape[0])) - 1 # 3x150\n",
    "synapse_1 = 2* np.random.random((y.shape[0], y.shape[1])) - 1 # 150x1\n",
    "\n",
    "# start training iterations (10k times)\n",
    "for i in range(100000):\n",
    "    # iteration start\n",
    "    num_iter = i\n",
    "    \n",
    "    # ---- training iteration step 1: Forward Propagation\n",
    "    # Forward Propagation applies weights of synapses in each layer to produce output layer\n",
    "    # Output layer is the layer that is the last layer as a final output for an iteration of training\n",
    "    # the initial layer is the input\n",
    "    l0 = X # 4x2\n",
    "    # the hidden layer after application of the first synapse weights to layer 0 values, which is the input\n",
    "    l0_weighted = np.dot(l0,synapse_0) # 4x2 2x4 -> 4x4\n",
    "    # smooth each weighted value into new value ranging [0, 1)\n",
    "    # now we generated the hidden layer, l1\n",
    "    l1 = nonlin(l0_weighted) # -> 4x4\n",
    "    \n",
    "    # we finally compute the output layer by applying the second synapse weights to layer 1 values\n",
    "    l1_weighted = np.dot(l1,synapse_1) # 4x4 4x1 -> 4x1\n",
    "    # smooth each weighted value into new value ranging [0, 1)\n",
    "    # now we generated the hidden layer, l1\n",
    "    l2 = nonlin(l1_weighted) # -> 4x1\n",
    "    # ---- Forward Propagation completed\n",
    "    \n",
    "    \n",
    "    # ---- training iteration step 2: Back Propagation\n",
    "    # Back Propagation adjust each weight of synapse layer by layer\n",
    "    \n",
    "    # First we figure out the initial error amount and direction on the output layer relative to the given output y\n",
    "    l2_error = y - l2 # 4x1 - 4x1 -> 4x1\n",
    "    # Incorporate the derivative of sigmoid on layer 2 (output layer)\n",
    "    l2_delta = l2_error * drv_nonlin(l2) # 4x1 4x1 -> 4x1\n",
    "    # Apply the delta computed to each synapses for the next iteration of training\n",
    "    synapse_1 += l1.T.dot(l2_delta) # 4x4.T(=4x4) 4x1 -> 4x1\n",
    "    \n",
    "    # How much weights of synapse 1 contributed to the l2_delta\n",
    "    l1_error = l2_delta.dot(synapse_1.T) # 4x1 4x1.T(=1x4) -> 4x4 \n",
    "    # Incorporate the derivative of sigmoid on layer 1 (hidden layer)\n",
    "    l1_delta = l1_error * drv_nonlin(l1) # 4x4 4x4 -> 4x4    \n",
    "    # Apply the delta computed to each synapses for the next iteration of training\n",
    "    synapse_0 += l0.T.dot(l1_delta) # 4x2.T(=2x4) 4x4 -> 2x4 \n",
    "    # ---- Back Propagation completed\n",
    "    \n",
    "    # Checking error of the output layer respects to given output for each 10000 iterations\n",
    "    # The number should decrease over the iterations\n",
    "    if (num_iter% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "\n",
    "print('')\n",
    "print(\"Output After Training:\")\n",
    "print(l2)\n",
    "\n",
    "\n",
    "print(\"This is success!\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# input dataset\n",
    "# X = np.array([ [0,1],\n",
    "#                [0,1],\n",
    "#                [1,0],\n",
    "#                [1,0] ])\n",
    "\n",
    "# output dataset\n",
    "# y = np.array([ [0],\n",
    "#                [0],\n",
    "#                [1],\n",
    "#                [1] ])\n",
    "# or simplify\n",
    "# y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# this is neccessary to observe the result under same random numbers in one session\n",
    "np.random.seed(1)\n",
    "\n",
    "# hidden layer configuration.\n",
    "# 3 layear: You need to configure synapse 0 and synapse 1 here.\n",
    "synapse_0 = 2* np.random.random((2, 4)) - 1 # 2x4\n",
    "synapse_1 = 2* np.random.random((4, 1)) - 1 # 4x1\n",
    "\n",
    "# start training iterations (10k times)\n",
    "for i in range(100000):\n",
    "    # iteration start\n",
    "    num_iter = i\n",
    "    \n",
    "    # ---- training iteration step 1: Forward Propagation\n",
    "    # Forward Propagation applies weights of synapses in each layer to produce output layer\n",
    "    # Output layer is the layer that is the last layer as a final output for an iteration of training\n",
    "    # the initial layer is the input\n",
    "    l0 = X # 4x2\n",
    "    # the hidden layer after application of the first synapse weights to layer 0 values, which is the input\n",
    "    l0_weighted = np.dot(l0,synapse_0) # 4x2 2x4 -> 4x4\n",
    "    # smooth each weighted value into new value ranging [0, 1)\n",
    "    # now we generated the hidden layer, l1\n",
    "    l1 = nonlin(l0_weighted) # -> 4x4\n",
    "    \n",
    "    # we finally compute the output layer by applying the second synapse weights to layer 1 values\n",
    "    l1_weighted = np.dot(l1,synapse_1) # 4x4 4x1 -> 4x1\n",
    "    # smooth each weighted value into new value ranging [0, 1)\n",
    "    # now we generated the hidden layer, l1\n",
    "    l2 = nonlin(l1_weighted) # -> 4x1\n",
    "    # ---- Forward Propagation completed\n",
    "    \n",
    "    \n",
    "    # ---- training iteration step 2: Back Propagation\n",
    "    # Back Propagation adjust each weight of synapse layer by layer\n",
    "    \n",
    "    # First we figure out the initial error amount and direction on the output layer relative to the given output y\n",
    "    l2_error = y - l2 # 4x1 - 4x1 -> 4x1\n",
    "    # Incorporate the derivative of sigmoid on layer 2 (output layer)\n",
    "    l2_delta = l2_error * drv_nonlin(l2) # 4x1 4x1 -> 4x1\n",
    "    # Apply the delta computed to each synapses for the next iteration of training\n",
    "    synapse_1 += l1.T.dot(l2_delta) # 4x4.T(=4x4) 4x1 -> 4x1\n",
    "    \n",
    "    # How much weights of synapse 1 contributed to the l2_delta\n",
    "    l1_error = l2_delta.dot(synapse_1.T) # 4x1 4x1.T(=1x4) -> 4x4 \n",
    "    # Incorporate the derivative of sigmoid on layer 1 (hidden layer)\n",
    "    l1_delta = l1_error * drv_nonlin(l1) # 4x4 4x4 -> 4x4    \n",
    "    # Apply the delta computed to each synapses for the next iteration of training\n",
    "    synapse_0 += l0.T.dot(l1_delta) # 4x2.T(=2x4) 4x4 -> 2x4 \n",
    "    # ---- Back Propagation completed\n",
    "    \n",
    "    # Checking error of the output layer respects to given output for each 10000 iterations\n",
    "    # The number should decrease over the iterations\n",
    "    if (num_iter% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "\n",
    "print('')\n",
    "print(\"Output After Training:\")\n",
    "print(l2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
