{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training With Alpha:1\n",
      "Error at 0 times: 0.503589968097\n",
      "Error at 1000 times: 0.0491140003698\n",
      "Error at 2000 times: 0.0285933488243\n",
      "Error at 3000 times: 0.0217416995946\n",
      "Error at 4000 times: 0.0180579295346\n",
      "Error at 5000 times: 0.0156816931897\n",
      "Error at 6000 times: 0.0139914002936\n",
      "Error at 7000 times: 0.012713147004\n",
      "Error at 8000 times: 0.0117050351321\n",
      "Error at 9000 times: 0.0108852190585\n",
      "Error at 10000 times: 0.0102027281294\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.01187053]\n",
      " [ 0.99163483]\n",
      " [ 0.99035002]\n",
      " [ 0.01092523]]\n",
      "\n",
      "Training With Alpha:10\n",
      "Error at 0 times: 0.503589968097\n",
      "Error at 1000 times: 0.252422788538\n",
      "Error at 2000 times: 0.251274160635\n",
      "Error at 3000 times: 0.251733180831\n",
      "Error at 4000 times: 0.0132009136641\n",
      "Error at 5000 times: 0.00839625941261\n",
      "Error at 6000 times: 0.00671413130865\n",
      "Error at 7000 times: 0.00576458226324\n",
      "Error at 8000 times: 0.00513254947463\n",
      "Error at 9000 times: 0.00467243365523\n",
      "Error at 10000 times: 0.00431791192321\n",
      "\n",
      "Output After Training:\n",
      "[[ 0.00478068]\n",
      " [ 0.99873911]\n",
      " [ 0.99387162]\n",
      " [ 0.00510169]]\n",
      "\n",
      "Training With Alpha:1000\n",
      "Error at 0 times: 0.503589968097\n",
      "Error at 1000 times: 0.499999999994\n",
      "Error at 2000 times: 0.499999999994\n",
      "Error at 3000 times: 0.499999999994\n",
      "Error at 4000 times: 0.499999999994\n",
      "Error at 5000 times: 0.499999999994\n",
      "Error at 6000 times: 0.499999999994\n",
      "Error at 7000 times: 0.499999999994\n",
      "Error at 8000 times: 0.499999999994\n",
      "Error at 9000 times: 0.499999999994\n",
      "Error at 10000 times: 0.499999999994\n",
      "\n",
      "Output After Training:\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid for binary problem\n",
    "def nonlin(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def drv_nonlin(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "# input dataset\n",
    "X = np.array([ [0,1,1],\n",
    "               [0,0,1],\n",
    "               [1,1,1],\n",
    "               [1,0,1] ])\n",
    "\n",
    "# output dataset\n",
    "# y = np.array([ [0],\n",
    "#                [1],\n",
    "#                [1],\n",
    "#                [0] ])\n",
    "# or simplify\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "alphas = [0.1, 10, 1000]\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(\"\\nTraining With Alpha:\" + str(alpha))\n",
    "\n",
    "    # seed random numbers to make calculation\n",
    "    # this is neccessary to observe the result under same random numbers in one session\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # hidden layer configuration.\n",
    "    # 3 layear: You need to configure synapse 0 and synapse 1 here.\n",
    "    synapse_0 = 2* np.random.random((3, 4)) - 1 # 3x4\n",
    "    synapse_1 = 2* np.random.random((4, 1)) - 1 # 4x1\n",
    "\n",
    "    # start training iterations (10k times)\n",
    "    for i in range(10001):\n",
    "        # iteration start\n",
    "        num_iter = i\n",
    "\n",
    "        # ---- training iteration step 1: Forward Propagation\n",
    "        # Forward Propagation applies weights of synapses in each layer to produce output layer\n",
    "        # Output layer is the layer that is the last layer as a final output for an iteration of training\n",
    "        # the initial layer is the input\n",
    "        l0 = X # 4x3\n",
    "        # the hidden layer after application of the first synapse weights to layer 0 values, which is the input\n",
    "        l0_weighted = np.dot(l0,synapse_0) # 4x3 3x4 -> 4x4\n",
    "        # smooth each weighted value into new value ranging [0, 1)\n",
    "        # now we generated the hidden layer, l1\n",
    "        l1 = nonlin(l0_weighted) # -> 4x4\n",
    "\n",
    "        # we finally compute the output layer by applying the second synapse weights to layer 1 values\n",
    "        l1_weighted = np.dot(l1,synapse_1) # 4x4 4x1 -> 4x1\n",
    "        # smooth each weighted value into new value ranging [0, 1)\n",
    "        # now we generated the hidden layer, l1\n",
    "        l2 = nonlin(l1_weighted) # -> 4x1\n",
    "        # ---- Forward Propagation completed\n",
    "\n",
    "\n",
    "        # ---- training iteration step 2: Back Propagation\n",
    "        # Back Propagation adjust each weight of synapse layer by layer\n",
    "\n",
    "        # First we figure out the initial error amount and direction on the output layer relative to the given output y\n",
    "        l2_error = y - l2# 4x1 - 4x1 -> 4x1\n",
    "        # Incorporate the derivative of sigmoid on layer 2 (output layer)\n",
    "        l2_delta = l2_error * drv_nonlin(l2) # 4x1 4x1 -> 4x1\n",
    "        # Apply the delta computed to each synapses for the next iteration of training\n",
    "        synapse_1 += alpha*l1.T.dot(l2_delta) # 4x4.T(=4x4) 4x1 -> 4x1\n",
    "\n",
    "        # How much weights of synapse 1 contributed to the l2_delta\n",
    "        l1_error = l2_delta.dot(synapse_1.T) # 4x1 4x1.T(=1x4) -> 4x4 \n",
    "        # Incorporate the derivative of sigmoid on layer 1 (hidden layer)\n",
    "        l1_delta = l1_error * drv_nonlin(l1) # 4x4 4x4 -> 4x4    \n",
    "        # Apply the delta computed to each synapses for the next iteration of training\n",
    "        synapse_0 += alpha*l0.T.dot(l1_delta) # 4x3.T(=2x4) 4x4 -> 3x4 \n",
    "        # ---- Back Propagation completed\n",
    "\n",
    "        # Checking error of the output layer respects to given output for each 10000 iterations\n",
    "        # The number should decrease over the iterations\n",
    "        if (num_iter% 1000) == 0:\n",
    "            print(\"Error at \" + str(num_iter) + \" times: \" + str(np.mean(np.abs(l2_error))))\n",
    "\n",
    "\n",
    "    print('')\n",
    "    print(\"Output After Training:\")\n",
    "    print(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
