{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Formula of Sigmoid function: $$S(t) = \\frac{1}{(1+e^t)}$$\n",
    "- Sigmoid function is the activation function that maps inputs to any numbesr between 0 to 1, which represents probabilities.\n",
    "- The process of making the simplified derivative of sigmoid:\n",
    "$$S(x) = \\frac{1}{(1+e^x)}$$\n",
    "$$f(x) = \\frac{1}{S(x)} = 1+e^x$$\n",
    "$$f'(x) = \\frac{S'(x)}{S(x)^2}$$\n",
    "$$f'(x) = -e^{-x} = 1 - f(x) = 1 - \\frac{1}{S(x)}\n",
    "= \\frac{(S(x)-1)}{S(x)}$$\n",
    "$$ \\frac{S'(x)}{S(x)^2} = \\frac{(S(x)-1)}{S(x)}$$ \n",
    "$$ S'(x) = {S(x)^2}\\frac{(S(x)-1)}{S(x)}$$ \n",
    "$$ S'(x) = {S(x)}(S(x)-1)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Input| Feature1 | Feature2 | Feature3\n",
    "| :-: | :-: |:-: | :-:\n",
    "| Sample1 | 0 | 0 | 1\n",
    "| Sample2 | 0 | 1 | 1\n",
    "| Sample3 | 1 | 0 | 1\n",
    "| Sample4 | 1 | 1 | 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Output\n",
    "| :-: \n",
    "|0 \n",
    "|0 \n",
    "|1 \n",
    "|1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since X is 4x3 matrix, to compute dot product, the synapse should be 3x1 matrix to produce 4x1 matrix for the next layer.\n",
    "- the matrix representation:\n",
    "\n",
    "$\\left[ \\begin{array}{cccc}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 1 \\\\ \\end{array} \\right]$ * $\\left[ \\begin{array}{cccc}\n",
    "s_1 \\\\\n",
    "s_2 \\\\\n",
    "s_3 \\\\ \\end{array} \\right]$ = $\\left[ \\begin{array}{cccc}\n",
    "o_1 \\\\\n",
    "o_2 \\\\\n",
    "o_3 \\\\\n",
    "o_4 \\\\ \\end{array} \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for iter in range(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X # layer 0\n",
    "    l1 = nonlin(np.dot(l0,syn0)) # layer 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward propagation is an operation that applies corresponding weights of a synapse onto each value of the input layer to produce the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The more error the row of output has, the more weight should be fixed to get better result in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The slope gives the direction to adjust the weights (positive or negative).\n",
    "- By multiplying it with error rate, it contains the direction and degree of the adjustment.\n",
    "- By taking dot product between layer 0 and the computed error information, update the weights at layer 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.2689864 ]\n",
      " [ 0.36375058]\n",
      " [ 0.23762817]\n",
      " [ 0.3262757 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Output After Training:\")\n",
    "print(l1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
